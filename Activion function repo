# activation_functions.py
import torch
import torch.nn as nn
import math

class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

class Mish(nn.Module):
    def forward(self, x):
        return x * torch.tanh(nn.functional.softplus(x))

class Sine(nn.Module):
    def forward(self, x):
        return torch.sin(x)

class Cosine(nn.Module):
    def forward(self, x):
        return torch.cos(x)

class Gaussian(nn.Module):
    def forward(self, x):
        return torch.exp(-x**2)

class Quadratic(nn.Module):
    def forward(self, x):
        return x**2

class Cubic(nn.Module):
    def forward(self, x):
        return x**3

class SoftExponential(nn.Module):
    def __init__(self, alpha=1.0):
        super().__init__()
        self.alpha = nn.Parameter(torch.tensor(alpha))

    def forward(self, x):
        if self.alpha == 0.0:
            return x
        elif self.alpha < 0:
            return -torch.log(1 - self.alpha * (x + self.alpha)) / self.alpha
        else:
            return (torch.exp(self.alpha * x) - 1) / self.alpha + self.alpha

class ParametricTanh(nn.Module):
    def __init__(self, scale=1.0):
        super().__init__()
        self.scale = nn.Parameter(torch.tensor(scale))

    def forward(self, x):
        return torch.tanh(self.scale * x)

activation_pool = {
    "relu": nn.ReLU(),
    "leaky_relu": nn.LeakyReLU(),
    "elu": nn.ELU(),
    "selu": nn.SELU(),
    "gelu": nn.GELU(),
    "sigmoid": nn.Sigmoid(),
    "tanh": nn.Tanh(),
    "swish": Swish(),
    "mish": Mish(),
    "sine": Sine(),
    "cosine": Cosine(),
    "gaussian": Gaussian(),
    "quadratic": Quadratic(),
    "cubic": Cubic(),
    "softplus": nn.Softplus(),
    "hardtanh": nn.Hardtanh(),
    "tanhshrink": nn.Tanhshrink(),
    "logsigmoid": nn.LogSigmoid(),
    "softsign": nn.Softsign(),
    "rrelu": nn.RReLU(),
    "prelu": nn.PReLU(),
    "threshold": nn.Threshold(0.1, 0.0),
    "sigmoid_hard": nn.Hardsigmoid(),
    "relu6": nn.ReLU6(),
    "softmin": nn.Softmin(dim=1),
    "softmax": nn.Softmax(dim=1),
    "logsoftmax": nn.LogSoftmax(dim=1),
    "soft_exponential": SoftExponential(),
    "parametric_tanh": ParametricTanh()
}

def get_activation(name):
    if name in activation_pool:
        return activation_pool[name]
    else:
        raise ValueError(f"Activation '{name}' not found in activation_pool")
