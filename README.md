# ðŸ”¬ KAN-NAFNet: A Hybrid Framework for Image Super-Resolution

This project integrates two powerful components:
- **KAN (Kernel Activation Network):** A dynamic multi-activation neural module with adaptive activation function selection.
- **NAFNet:** A lightweight convolutional architecture for image enhancement and denoising.

The integration enables patch-wise image super-resolution, guided by learned activation performance and perceptual loss optimization.

---

## ðŸš€ Features

âœ… **Patch-based super-resolution** with learned KAN upscaling  
âœ… **Dynamic activation selection** from over 30 activation functions  
âœ… **Fusion with NAFNet** for feature refinement and detail restoration  
âœ… **Custom dataset support** with automatic HRâ†’LR conversion  
âœ… **Support for advanced loss functions:** Charbonnier & Perceptual Loss  
âœ… **Activation performance tracking** for adaptive training strategies  

---

## ðŸ§  Architecture Overview

| Module | Purpose |
|--------|---------|
| `ActivationModule` | Provides dozens of activation functions, including Swish, Mish, SQNL, ISRLU, etc. |
| `KANLinear` | Linear layer with multiple parallel activation paths and learned weights |
| `KANNetwork` | A stack of `KANLinear` layers forming a flexible MLP |
| `KANeural` | Processes image patches and upsamples them using KAN logic |
| `NAFNet` | Lightweight image restoration network with residual and gating mechanisms |
| `KANNAFNetIntegrated` | Full hybrid model combining multiple KAN blocks + NAFNet |
| `SRDataset` | Custom dataset that dynamically creates LR images from HR originals |
| `CharbonnierLoss` | Smooth L1 loss for robustness to outliers |
| `PerceptualLoss` | Uses VGG19 features to optimize visual quality |
| `DynamicActivationRegistry` | Tracks activation performance and recommends top activations |

---

## ðŸ“¦ Dependencies

```bash
pip install torch torchvision numpy pillow matplotlib tqdm h5py pyyaml
```

---

## ðŸ’¡ Example Usage

```python
from model import KANNAFNetIntegrated
import torch

model = KANNAFNetIntegrated(in_channels=3)
input_tensor = torch.randn(1, 3, 64, 64)
output = model(input_tensor)
print(output.shape)  # Should output high-resolution image tensor
```

---

## ðŸ§ª Dataset Preparation

Your high-resolution image folder should contain `.png`, `.jpg`, or `.bmp` files:

```python
train_loader, val_loader = create_dataloaders(
    hr_dir='path/to/HR', 
    batch_size=8, 
    patch_size=128,
    lr_scale=4
)
```

Each HR image will be downscaled using bicubic interpolation to simulate LR inputs.

---

## ðŸŽ¯ Training Objective

You can combine multiple loss functions:

```python
loss1 = CharbonnierLoss()(output, target)
loss2 = PerceptualLoss()(output, target)
total_loss = loss1 + 0.1 * loss2
```

---

## ðŸ“ˆ Activation Tracking

Use the `DynamicActivationRegistry` to keep track of the most successful activation functions and dynamically select the top-performing ones during training.

```python
registry = DynamicActivationRegistry()
registry.update_metrics("swish", 0.03)
top_acts = registry.get_top_activations(n=5)
```

---

## ðŸ§± Applications

- Super-resolution of natural images (x2, x4 scaling)
- Denoising and artifact reduction
- Activation function analysis and dynamic architecture tuning

---

## ðŸ“Œ Future Improvements

- Add support for attention-based fusion between KAN and NAFNet
- Learnable activation function replacement schedules
- Benchmarking on standard datasets like DIV2K and Set5

---

Super-Resolution Pipeline
â”‚
â”œâ”€â”€ ðŸ“¥ Load High-Resolution (HR) Images
â”‚   â””â”€â”€ from user-specified folder (e.g., ./data/HR_Images)
â”‚
â”œâ”€â”€ ðŸ”„ Preprocessing
â”‚   â”œâ”€â”€ Apply random flip, rotation (for data augmentation)
â”‚   â””â”€â”€ Extract HR patches and downscale to get LR patches
â”‚
â”œâ”€â”€ ðŸ§  Model Architecture
â”‚   â”œâ”€â”€ ðŸ”— KAN Blocks
â”‚   â”‚   â””â”€â”€ Approximate HR pixel functions using adaptive activation networks
â”‚   â”œâ”€â”€ ðŸ”§ NAFNet Blocks
â”‚   â”‚   â””â”€â”€ Denoise and resolve pixel congestion in upsampled images
â”‚   â””â”€â”€ ðŸ”€ Combined Output
â”‚       â””â”€â”€ Clean, high-quality SR image
â”‚
â”œâ”€â”€ ðŸŽ¯ Loss Functions
â”‚   â”œâ”€â”€ Charbonnier Loss (pixel-wise robust loss)
â”‚   â””â”€â”€ Perceptual Loss (feature-level comparison via VGG)
â”‚
â”œâ”€â”€ ðŸ“š Training Phase
â”‚   â”œâ”€â”€ Loop over epochs and batches
â”‚   â”œâ”€â”€ Compute losses and update model weights
â”‚   â””â”€â”€ Periodically update activation function usage stats
â”‚
â”œâ”€â”€ ðŸ“ˆ Validation
â”‚   â””â”€â”€ Compare predicted SR images with HR ground truth (using unseen LR inputs)
â”‚
â””â”€â”€ ðŸ’¾ Output
    â”œâ”€â”€ Save trained model
    â”œâ”€â”€ Log losses and performance
    â””â”€â”€ Export SR validation samples


























## ðŸ“œ License

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT).

